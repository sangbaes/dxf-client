import base64
import json
import time
import uuid
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request
from io import BytesIO
from datetime import datetime, timezone, timedelta

import streamlit as st

def reset_for_new_job():
    """Reset session state so the client app is ready for a new batch/job."""
    for k in [
        "active_job_id",
        "active_job_ids",
        "active_batch_id",
        "upload_progress",
        "selected_manifest",
        "zip_bytes",
        "zip_name",
    ]:
        st.session_state.pop(k, None)

    # Reset file uploader by bumping its key
    st.session_state["uploader_key"] = st.session_state.get("uploader_key", 0) + 1

from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
import socket
import ssl
import httplib2
from google_auth_httplib2 import AuthorizedHttp
from googleapiclient.errors import HttpError


# =========================
# Config
# =========================
DXF_SHARED_FOLDER_ID = "1qhx_xTGdOusxhV0xN2df4Kc8JTfh3zTd"

SUBFOLDERS = ["INBOX", "WORKING", "DONE", "META"]
MAX_FILE_MB = 200
MAX_FILE_BYTES = MAX_FILE_MB * 1024 * 1024

SEOUL_TZ = timezone(timedelta(hours=9))

SCOPES = ["https://www.googleapis.com/auth/drive"]


# =========================
# Helpers
# =========================
def now_seoul_iso() -> str:
    return datetime.now(SEOUL_TZ).isoformat(timespec="seconds")


def make_job_id(original_name: str) -> str:
    ts = datetime.now(SEOUL_TZ).strftime("%Y%m%d_%H%M%S")
    short = uuid.uuid4().hex[:8]
    safe = "".join(c for c in original_name if c.isalnum() or c in ("-", "_", "."))
    safe = safe[:40] if safe else "file"
    return f"{ts}_{short}_{safe}"



def drive_execute(req, retries: int = 5, base_sleep: float = 0.6):
    """Drive API ìš”ì²­ì„ ë„¤íŠ¸ì›Œí¬ í”ë“¤ë¦¼ì—ë„ ìµœëŒ€í•œ ê²¬ë””ë„ë¡ ì¬ì‹œë„ ì‹¤í–‰."""
    last_err = None
    for i in range(retries + 1):
        try:
            # googleapiclient ìì²´ ì¬ì‹œë„ë„ ìˆì§€ë§Œ, SSL read/connection resetì€ ì§ì ‘ ê°ì‹¸ì£¼ëŠ” ê²Œ ë” ì•ˆì •ì ì„
            return req.execute(num_retries=1)
        except (HttpError, OSError, ssl.SSLError, socket.timeout) as e:
            last_err = e
            if i >= retries:
                raise
            time.sleep(base_sleep * (2 ** i))
    raise last_err


def load_service_account_info():
    # âœ… Base64 ë°©ì‹ (Streamlit Secrets: SERVICE_ACCOUNT_B64)
    if "SERVICE_ACCOUNT_B64" not in st.secrets:
        raise RuntimeError("Streamlit Secretsì— SERVICE_ACCOUNT_B64ê°€ ì—†ìŠµë‹ˆë‹¤.")

    raw = base64.b64decode(st.secrets["SERVICE_ACCOUNT_B64"].encode("ascii"))
    info = json.loads(raw.decode("utf-8"))

    # ë°©ì–´: í˜¹ì‹œ \\në¡œ ì €ì¥ëœ ê²½ìš° ì‹¤ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³µêµ¬
    if "private_key" in info and isinstance(info["private_key"], str):
        info["private_key"] = info["private_key"].replace("\\n", "\n").strip()

    return info

    if "SERVICE_ACCOUNT_JSON" in st.secrets:
        info = json.loads(st.secrets["SERVICE_ACCOUNT_JSON"])
        if "private_key" in info and isinstance(info["private_key"], str):
            info["private_key"] = info["private_key"].replace("\\n", "\n").strip()
        return info

    raise RuntimeError(
        "Streamlit Secretsì— gcp_service_account ë˜ëŠ” SERVICE_ACCOUNT_JSONì´ ì—†ìŠµë‹ˆë‹¤."
    )

SCOPES = ["https://www.googleapis.com/auth/drive"]

@st.cache_resource(show_spinner=False)
def get_drive_service():
    s = st.secrets["drive_oauth"]
    creds = Credentials(
        token=None,
        refresh_token=s["refresh_token"],
        token_uri="https://oauth2.googleapis.com/token",
        client_id=s["client_id"],
        client_secret=s["client_secret"],
        scopes=SCOPES,
    )
    # access_tokenì´ í•„ìš”í•  ë•Œ ìë™ ê°±ì‹ 
    creds.refresh(Request())

    # Streamlit Cloudì—ì„œ ê°„í—ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” SSL/ë„¤íŠ¸ì›Œí¬ read ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´
    # - httplib2 timeout ì§€ì •
    # - AuthorizedHttp ì‚¬ìš©
    authed_http = AuthorizedHttp(creds, http=httplib2.Http(timeout=30))
    return build("drive", "v3", http=authed_http, cache_discovery=False)


def find_or_create_folder(drive, parent_id: str, name: str) -> str:
    q = (
        f"'{parent_id}' in parents and "
        f"name = '{name}' and "
        "mimeType = 'application/vnd.google-apps.folder' and "
        "trashed = false"
    )
    res = drive.files().list(q=q, fields="files(id,name)").execute(num_retries=3)
    files = res.get("files", [])
    if files:
        return files[0]["id"]

    metadata = {
        "name": name,
        "mimeType": "application/vnd.google-apps.folder",
        "parents": [parent_id],
    }
    folder = drive.files().create(body=metadata, fields="id").execute()
    return folder["id"]


def get_subfolder_ids(drive):
    # cache in session to avoid repeated API calls
    if "subfolder_ids" in st.session_state:
        return st.session_state["subfolder_ids"]

    ids = {}
    for name in SUBFOLDERS:
        ids[name] = find_or_create_folder(drive, DXF_SHARED_FOLDER_ID, name)

    st.session_state["subfolder_ids"] = ids
    return ids


def upload_file_to_folder(drive, folder_id: str, filename: str, file_bytes: bytes, mime: str):
    media = MediaIoBaseUpload(BytesIO(file_bytes), mimetype=mime, resumable=True)
    metadata = {"name": filename, "parents": [folder_id]}
    req = drive.files().create(body=metadata, media_body=media, fields="id,name,size,createdTime")
    resp = None

    # Resumable upload loop
    while resp is None:
        status, resp = req.next_chunk()
        if status:
            st.session_state["upload_progress"] = int(status.progress() * 100)

    return resp


def upsert_json_file(drive, folder_id: str, filename: str, payload: dict):
    # Find existing
    q = (
        f"'{folder_id}' in parents and "
        f"name = '{filename}' and "
        "mimeType != 'application/vnd.google-apps.folder' and "
        "trashed = false"
    )
    res = drive.files().list(q=q, fields="files(id,name)").execute()
    files = res.get("files", [])

    data = json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")
    media = MediaIoBaseUpload(BytesIO(data), mimetype="application/json", resumable=False)

    if files:
        file_id = files[0]["id"]
        updated = drive.files().update(fileId=file_id, media_body=media).execute()
        return updated
    else:
        meta = {"name": filename, "parents": [folder_id]}
        created = drive.files().create(body=meta, media_body=media, fields="id").execute()
        return created


def read_json_file_by_name(drive, folder_id: str, filename: str) -> dict | None:
    q = (
        f"'{folder_id}' in parents and "
        f"name = '{filename}' and "
        "trashed = false"
    )
    res = drive.files().list(q=q, fields="files(id,name)").execute()
    files = res.get("files", [])
    if not files:
        return None

    file_id = files[0]["id"]
    request = drive.files().get_media(fileId=file_id)

    buf = BytesIO()
    downloader = MediaIoBaseDownload(buf, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()

    buf.seek(0)
    return json.loads(buf.read().decode("utf-8"))


def list_recent_jobs(drive, meta_folder_id: str, limit: int = 20):
    q = f"'{meta_folder_id}' in parents and trashed=false"
    req = drive.files().list(
        q=q,
        fields="files(id,name,createdTime,modifiedTime,size)",
        orderBy="modifiedTime desc",
        pageSize=limit,
    )

    try:
        res = drive_execute(req, retries=5)
    except Exception as e:
        # Drive APIê°€ ì¼ì‹œì ìœ¼ë¡œ í”ë“¤ë¦´ ë•Œ ì•±ì´ ì£½ì§€ ì•Šê²Œ ë°©ì–´
        st.warning(
            "Google Drive ì¡°íšŒê°€ ì¼ì‹œì ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.\n"
            f"ì›ì¸: {type(e).__name__}"
        )
        return []

    files = res.get("files", [])
    files = [f for f in files if f.get("name", "").lower().endswith(".json")]
    return files


def download_file_bytes(drive, file_id: str) -> bytes:
    request = drive.files().get_media(fileId=file_id)
    buf = BytesIO()
    downloader = MediaIoBaseDownload(buf, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    return buf.getvalue()


def find_file_in_folder_by_name(drive, folder_id: str, filename: str):
    q = (
        f"'{folder_id}' in parents and "
        f"name = '{filename}' and "
        "trashed = false"
    )
    res = drive.files().list(q=q, fields="files(id,name,size,modifiedTime)").execute()
    files = res.get("files", [])
    return files[0] if files else None


# =========================
# UI
# =========================
st.set_page_config(page_title="DXF Client", layout="centered")

# Google Analytics
st.components.v1.html(
    """
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E1LFDTNPVP"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-E1LFDTNPVP');
    </script>
    """,
    height=0,
)

st.title("DXF Client")


col_new1, col_new2 = st.columns([1, 3])
with col_new1:
    if st.button("ğŸ†• ìƒˆ ì‘ì—… ì‹œì‘"):
        reset_for_new_job()
        st.rerun()
with col_new2:
    st.caption("ìƒˆ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì´ì „ ì‘ì—… ì¬ë‹¤ìš´ë¡œë“œëŠ” ëª¨ë‹ˆí„°ë§ ì•±ì—ì„œ ì œê³µ ì˜ˆì •)")

with st.expander("ì„¤ëª…", expanded=False):
    st.markdown(
        """
- ì´ ì•±ì€ **Google Drive ê³µìœ í´ë”ì— DXFë¥¼ ì—…ë¡œë“œ**í•˜ê³ ,
- MacBook Pro ë¡œì»¬ ì›Œì»¤ê°€ ë²ˆì—­ í›„ ê²°ê³¼ë¥¼ `DONE/`ì— ì˜¬ë¦¬ë©´,
- ì•±ì´ ì™„ë£Œë¥¼ ê°ì§€í•´ **ë‹¤ìš´ë¡œë“œ ë²„íŠ¼**ì„ ì œê³µí•©ë‹ˆë‹¤.
        """
    )

# Drive connection
try:
    drive = get_drive_service()
    folders = get_subfolder_ids(drive)
except Exception as e:
    st.error("Google Drive ì—°ê²°/í´ë” ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Secrets ë˜ëŠ” í´ë” ê³µìœ  ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
    st.exception(e)
    st.stop()

st.success("âœ… Google Drive ì—°ê²°ë¨")
st.caption(f"DXF_SHARED_FOLDER_ID = {DXF_SHARED_FOLDER_ID}")

# Sidebar controls
st.sidebar.header("ì˜µì…˜")
auto_refresh = st.sidebar.checkbox("ìƒíƒœ ìë™ ìƒˆë¡œê³ ì¹¨", value=True)
refresh_sec = st.sidebar.slider("ìƒˆë¡œê³ ì¹¨ ì£¼ê¸°(ì´ˆ)", 3, 30, 5)

st.sidebar.divider()
st.sidebar.caption("í´ë”")
for k in SUBFOLDERS:
    st.sidebar.write(f"- {k}: `{folders[k]}`")

# Upload section
st.subheader("1) DXF ì—…ë¡œë“œ")
uploaded_list = uploaded_list = st.file_uploader(
    "DXF íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
    type=["dxf"],
    accept_multiple_files=True,
    key=f"uploader_{st.session_state.get('uploader_key', 0)}",
)
", type=["dxf"], accept_multiple_files=True, key=f"uploader_{st.session_state.get('uploader_key', 0)}")

if uploaded_list is not None:
    size = uploaded_list.size  # bytes
    st.write(f"íŒŒì¼ëª…: `{uploaded_list.name}` / í¬ê¸°: {size/1024/1024:.1f} MB")

    if size > MAX_FILE_BYTES:
        st.error(f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. {MAX_FILE_MB}MB ì´í•˜ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        if st.button("INBOXë¡œ ì—…ë¡œë“œ", type="primary"):
            st.session_state.pop("upload_progress", None)
            file_bytes = uploaded_list.getvalue()

            job_id = make_job_id(uploaded_list.name)
            inbox_name = f"{job_id}__{uploaded_list.name}"

            meta_filename = f"{job_id}.json"
            meta_payload = {
                "job_id": job_id,
                "original_name": uploaded_list.name,
                "inbox_name": inbox_name,
                "status": "queued",  # queued | working | done | error
                "created_at": now_seoul_iso(),
                "updated_at": now_seoul_iso(),
                "progress": 0,
                "message": "Uploaded to INBOX. Waiting for local worker.",
                "done_file": None,
                "error": None,
            }

            with st.spinner("ì—…ë¡œë“œ ì¤‘..."):
                try:
                    resp = upload_file_to_folder(
                        drive,
                        folders["INBOX"],
                        inbox_name,
                        file_bytes,
                        mime="application/dxf"
                    )
                    meta_payload["inbox_file_id"] = resp.get("id")
                    meta_payload["progress"] = 5
                    meta_payload["updated_at"] = now_seoul_iso()

                    upsert_json_file(drive, folders["META"], meta_filename, meta_payload)

                    st.success("âœ… ì—…ë¡œë“œ ì™„ë£Œ")
                    st.code(f"job_id: {job_id}")
                    st.session_state["active_job_id"] = job_id

                except Exception as e:
                    st.error("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨")
                    st.exception(e)

# Progress indicator (upload)
if "upload_progress" in st.session_state:
    st.progress(st.session_state["upload_progress"] / 100.0)

st.divider()

# Job monitor
st.subheader("2) ì‘ì—… ìƒíƒœ / ë‹¤ìš´ë¡œë“œ")

# Load recent jobs for selection
recent = list_recent_jobs(drive, folders["META"], limit=30)
recent_ids = [f["name"].replace(".json", "") for f in recent]

default_job = st.session_state.get("active_job_id")
if default_job and default_job in recent_ids:
    default_index = recent_ids.index(default_job)
else:
    default_index = 0 if recent_ids else None

job_id = None
if recent_ids:
    job_id = st.selectbox("ìµœê·¼ ì‘ì—… ì„ íƒ", recent_ids, index=default_index)
else:
    st.info("META í´ë”ì— ì‘ì—…ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")

# Auto refresh
def do_autorefresh():
    # streamlit has st_autorefresh in many versions
    try:
        from streamlit import st_autorefresh
        st_autorefresh(interval=refresh_sec * 1000, key="job_poll")
    except Exception:
        # fallback: user can press button
        pass

if auto_refresh:
    do_autorefresh()

col_a, col_b = st.columns([1, 1])
with col_a:
    manual_refresh = st.button("ìƒíƒœ ìƒˆë¡œê³ ì¹¨")
with col_b:
    st.caption("ìë™ ìƒˆë¡œê³ ì¹¨ì´ ì•ˆ ë˜ë©´ ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

if job_id:
    meta_name = f"{job_id}.json"
    meta = None
    try:
        meta = read_json_file_by_name(drive, folders["META"], meta_name)
    except Exception as e:
        st.error("META ì½ê¸° ì‹¤íŒ¨")
        st.exception(e)

    if meta:
        st.write(f"**status:** `{meta.get('status')}`")
        st.write(f"**updated_at:** `{meta.get('updated_at')}`")
        st.write(f"**message:** {meta.get('message')}")
        prog = int(meta.get("progress", 0) or 0)
        st.progress(min(max(prog, 0), 100) / 100.0)

        if meta.get("status") == "error":
            st.error("ì‘ì—… ì‹¤íŒ¨")
            if meta.get("error"):
                st.code(meta.get("error"))

        if meta.get("status") == "done":
            done_file = meta.get("done_file")
            if not done_file:
                st.warning("done ìƒíƒœì§€ë§Œ done_file ì •ë³´ê°€ METAì— ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success("âœ… ë²ˆì—­ ì™„ë£Œ")
                st.write(f"ê²°ê³¼ íŒŒì¼: `{done_file}`")

                done_obj = find_file_in_folder_by_name(drive, folders["DONE"], done_file)
                if not done_obj:
                    st.warning("DONE í´ë”ì—ì„œ ê²°ê³¼ íŒŒì¼ì„ ì•„ì§ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                else:
                    # Download through API and offer download button
                    try:
                        with st.spinner("ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘... (íŒŒì¼ì´ í¬ë©´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                            data = download_file_bytes(drive, done_obj["id"])
                        st.download_button(
                            label="ê²°ê³¼ DXF ë‹¤ìš´ë¡œë“œ",
                            data=data,
                            file_name=done_file,
                            mime="application/dxf",
                            type="primary",
                        )
                    except Exception as e:
                        st.error("ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì‹¤íŒ¨")
                        st.exception(e)

    else:
        st.info("í•´ë‹¹ jobì˜ META íŒŒì¼ì„ ì•„ì§ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")