import base64
import json
import time
import uuid
import zipfile
from io import BytesIO
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import streamlit as st
import httplib2
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google_auth_httplib2 import AuthorizedHttp


from googleapiclient.errors import HttpError

def _http_error_details(e: HttpError) -> str:
    try:
        return (getattr(e, "content", b"") or b"").decode("utf-8", errors="ignore")
    except Exception:
        return ""

def execute_with_retries(request, retries: int = 6, base_sleep: float = 1.0):
    """Execute a googleapiclient request with exponential backoff.
    Retries transient errors (429/5xx/409), some 403 rate-limit cases, and common network errors.
    """
    last_err = None
    for i in range(retries):
        try:
            return request.execute()
        except (BrokenPipeError, ConnectionError, TimeoutError, OSError) as e:
            last_err = e
            if i == retries - 1:
                raise
            time.sleep(base_sleep * (2 ** i))
        except HttpError as e:
            last_err = e
            status = getattr(e.resp, "status", None)
            body = _http_error_details(e)
            retryable = status in (429, 500, 502, 503, 504, 409)
            if status == 403 and ("rateLimitExceeded" in body or "userRateLimitExceeded" in body):
                retryable = True
            if (not retryable) or (i == retries - 1):
                raise
            time.sleep(base_sleep * (2 ** i))
    raise last_err

# =========================================================
# Config
# =========================================================
SEOUL_TZ = datetime.now().astimezone().tzinfo  # Streamlit Cloudì—ì„œë„ ë¡œì»¬ tzê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ ê°„ë‹¨íˆ
SCOPES = ["https://www.googleapis.com/auth/drive"]

MAX_FILE_MB = 50
MAX_FILE_BYTES = MAX_FILE_MB * 1024 * 1024

INBOX_FOLDER_ID = st.secrets.get("INBOX_FOLDER_ID", "1QFhwS0aMPbwjtpC0k83ZJr8-abHksNhJ")
DONE_FOLDER_ID  = st.secrets.get("DONE_FOLDER_ID",  "1rC_1x1HAoJZ65YuGLDw8GikyBbqXWIJa")
META_FOLDER_ID  = st.secrets.get("META_FOLDER_ID",  "1x2YCQTPOd5KC4tZdwfmX8zf7NZNO6Y_w")

SUBFOLDERS = {"INBOX": INBOX_FOLDER_ID, "DONE": DONE_FOLDER_ID, "META": META_FOLDER_ID}

# =========================================================
# Session reset
# =========================================================
def reset_for_new_job():
    for k in [
        "active_job_id",
        "active_job_ids",
        "active_batch_id",
        "upload_progress",
        "selected_manifest_id",
        "selected_manifest_name",
        "zip_bytes",
        "zip_name",
    ]:
        st.session_state.pop(k, None)
    st.session_state["uploader_key"] = st.session_state.get("uploader_key", 0) + 1

# =========================================================
# Helpers
# =========================================================
def now_seoul_iso() -> str:
    # ISO seconds
    return datetime.now().isoformat(timespec="seconds")

def make_job_id(original_name: str) -> str:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    short = uuid.uuid4().hex[:8]
    safe = "".join(c for c in original_name if c.isalnum() or c in ("-", "_", "."))
    safe = safe[:40] if safe else "file"
    return f"{ts}_{short}_{safe}"

def make_batch_id() -> str:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{ts}_{uuid.uuid4().hex[:8]}"

def get_service_account_info() -> dict:
    # 1) Streamlit í‘œì¤€ ë°©ì‹: [gcp_service_account]
    if "gcp_service_account" in st.secrets:
        info = dict(st.secrets["gcp_service_account"])
        if "private_key" in info and isinstance(info["private_key"], str):
            info["private_key"] = info["private_key"].replace("\\n", "\n").strip()
        return info

    # 2) ìš°ë¦¬ê°€ ì“°ë˜ B64/JSON ë°©ì‹
    if "SERVICE_ACCOUNT_B64" in st.secrets:
        raw = base64.b64decode(st.secrets["SERVICE_ACCOUNT_B64"].encode("ascii"))
        info = json.loads(raw.decode("utf-8"))
        if "private_key" in info and isinstance(info["private_key"], str):
            info["private_key"] = info["private_key"].replace("\\n", "\n").strip()
        return info

    if "SERVICE_ACCOUNT_JSON" in st.secrets:
        info = json.loads(st.secrets["SERVICE_ACCOUNT_JSON"])
        if "private_key" in info and isinstance(info["private_key"], str):
            info["private_key"] = info["private_key"].replace("\\n", "\n").strip()
        return info

    st.error("âŒ Google Drive ì„œë¹„ìŠ¤ê³„ì • Secretsê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.info("Streamlit Cloud â†’ Settings â†’ Secretsì— SERVICE_ACCOUNT_JSON ë˜ëŠ” [gcp_service_account]ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()



@st.cache_resource(show_spinner=False)
def get_drive():
    info = get_service_account_info()
    creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    # httplib2 timeout í™•ì¥ (SSL read ëŠê¹€ ì™„í™”)
    authed_http = AuthorizedHttp(creds, http=httplib2.Http(timeout=90))
    return build("drive", "v3", http=authed_http, cache_discovery=False)

def upload_file_to_folder(drive, folder_id: str, filename: str, file_bytes: bytes, mime: str):
    media = MediaIoBaseUpload(BytesIO(file_bytes), mimetype=mime, resumable=False)
    body = {"name": filename, "parents": [folder_id]}
    return execute_with_retries(drive.files().create(body=body, media_body=media, fields=\"id,name\", supportsAllDrives=True))

def download_file_bytes(drive, file_id: str) -> bytes:
    request = drive.files().get_media(fileId=file_id, supportsAllDrives=True)
    fh = BytesIO()
    downloader = MediaIoBaseDownload(fh, request, chunksize=1024 * 1024)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    return fh.getvalue()

def upsert_json_file(drive, folder_id: str, filename: str, payload: dict):
    q = (
        f"'{folder_id}' in parents and "
        f"name = '{filename}' and "
        "mimeType != 'application/vnd.google-apps.folder' and "
        "trashed = false"
    )
    res = execute_with_retries(drive.files().list(q=q, fields=\"files(id,name)\", supportsAllDrives=True, includeItemsFromAllDrives=True))
    files = res.get("files", [])
    data = json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")
    media = MediaIoBaseUpload(BytesIO(data), mimetype="application/json", resumable=False)

    if files:
        file_id = files[0]["id"]
        return execute_with_retries(drive.files().update(fileId=file_id, media_body=media, fields=\"id,name\", supportsAllDrives=True))
    else:
        body = {"name": filename, "parents": [folder_id]}
        return execute_with_retries(drive.files().create(body=body, media_body=media, fields=\"id,name\", supportsAllDrives=True))

def find_file_by_name(drive, folder_id: str, filename: str) -> Optional[dict]:
    q = (
        f"'{folder_id}' in parents and "
        f"name = '{filename}' and "
        "mimeType != 'application/vnd.google-apps.folder' and "
        "trashed = false"
    )
    res = execute_with_retries(drive.files().list(q=q, fields=\"files(id,name,modifiedTime,size)\", supportsAllDrives=True, includeItemsFromAllDrives=True))
    files = res.get("files", [])
    return files[0] if files else None

def list_manifest_files(drive, folder_id: str, limit: int = 50) -> List[dict]:
    # name contains '__manifest.json'
    q = (
        f"'{folder_id}' in parents and "
        "name contains '__manifest.json' and "
        "trashed = false"
    )
    res = execute_with_retries(drive.files().list(q=q, pageSize=min(limit, 100), fields=\"files(id,name,modifiedTime),nextPageToken\", orderBy=\"modifiedTime desc\", supportsAllDrives=True, includeItemsFromAllDrives=True))
    return res.get("files", [])

def read_json_file_by_id(drive, file_id: str) -> dict:
    raw = download_file_bytes(drive, file_id)
    return json.loads(raw.decode("utf-8"))

def safe_basename(name: str) -> str:
    # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œë¬¸ì ì œê±°
    return name.split("/")[-1].split("\\")[-1]

# =========================================================
# UI
# =========================================================
st.set_page_config(page_title="DXF Client", layout="wide")
st.title("DXF Client (Batch Upload + ZIP Download)")

col_new1, col_new2 = st.columns([1, 3])
with col_new1:
    if st.button("ğŸ†• ìƒˆ ì‘ì—… ì‹œì‘"):
        reset_for_new_job()
        st.rerun()
with col_new2:
    st.caption("ìƒˆ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì§€ë‚œ ì‘ì—… ì¬ë‹¤ìš´ë¡œë“œ/íˆìŠ¤í† ë¦¬ëŠ” ëª¨ë‹ˆí„°ë§ ì•±ì—ì„œ ì œê³µ)")

drive = get_drive()
folders = SUBFOLDERS

# Sidebar controls
st.sidebar.header("ì˜µì…˜")
auto_refresh = st.sidebar.checkbox("ìƒíƒœ ìë™ ìƒˆë¡œê³ ì¹¨", value=True)
refresh_sec = st.sidebar.slider("ìƒˆë¡œê³ ì¹¨ ì£¼ê¸°(ì´ˆ)", 3, 30, 5)

st.sidebar.divider()
st.sidebar.caption("í´ë”")
for k, v in folders.items():
    st.sidebar.write(f"- {k}: `{v}`")

# =========================================================
# 1) Upload (Batch)
# =========================================================
st.subheader("1) DXF ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ)")
uploaded_list = st.file_uploader(
    "DXF íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
    type=["dxf"],
    accept_multiple_files=True,
    key=f"uploader_{st.session_state.get('uploader_key', 0)}",
)

if uploaded_list:
    total_files = len(uploaded_list)
    total_size = sum(u.size for u in uploaded_list)
    st.write(f"ì„ íƒëœ íŒŒì¼: **{total_files}ê°œ** / ì´ í¬ê¸°: **{total_size/1024/1024:.1f} MB**")

    too_big = [u for u in uploaded_list if u.size > MAX_FILE_BYTES]
    if too_big:
        st.error(
            "ì•„ë˜ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. "
            f"{MAX_FILE_MB}MB ì´í•˜ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n- "
            + "\n- ".join([f"{u.name} ({u.size/1024/1024:.1f}MB)" for u in too_big])
        )
    else:
        if st.button("INBOXë¡œ ì¼ê´„ ì—…ë¡œë“œ", type="primary"):
            batch_id = make_batch_id()
            created_at = now_seoul_iso()

            manifest_name = f"{batch_id}__manifest.json"
            manifest_payload = {
                "batch_id": batch_id,
                "status": "uploading",  # uploading | queued | done | error
                "created_at": created_at,
                "updated_at": created_at,
                "total": total_files,
                "items": [],
                "message": "Uploading files to INBOX and writing META items."
            }

            prog = st.progress(0)
            status_box = st.empty()

            ok_count = 0
            errors = []

            with st.spinner("ì—…ë¡œë“œ ì¤‘..."):
                for idx, up in enumerate(uploaded_list, 1):
                    try:
                        orig_name = safe_basename(up.name)
                        file_bytes = up.getvalue()

                        job_id = make_job_id(orig_name)
                        inbox_name = f"{job_id}__{orig_name}"
                        meta_filename = f"{job_id}.json"

                        meta_payload = {
                            "batch_id": batch_id,
                            "job_id": job_id,
                            "original_name": orig_name,
                            "inbox_name": inbox_name,
                            "status": "queued",
                            "created_at": created_at,
                            "updated_at": now_seoul_iso(),
                            "progress": 0,
                            "message": "Uploaded to INBOX. Waiting for local worker.",
                            "done_file": None,
                            "error": None,
                        }

                        resp = upload_file_to_folder(
                            drive,
                            folders["INBOX"],
                            inbox_name,
                            file_bytes,
                            mime="application/dxf"
                        )
                        meta_payload["inbox_file_id"] = resp.get("id")
                        meta_payload["progress"] = 5
                        meta_payload["updated_at"] = now_seoul_iso()

                        upsert_json_file(drive, folders["META"], meta_filename, meta_payload)

                        manifest_payload["items"].append({
                            "job_id": job_id,
                            "meta_filename": meta_filename,
                            "original_name": orig_name,
                            "inbox_name": inbox_name,
                            "inbox_file_id": meta_payload.get("inbox_file_id"),
                            "status": "queued",
                        })
                        ok_count += 1

                    except Exception as e:
                        errors.append({"file": up.name, "error": str(e)})

                    pct = int((idx / total_files) * 100)
                    prog.progress(pct)
                    status_box.write(f"ì—…ë¡œë“œ ì§„í–‰: {idx}/{total_files} (ì„±ê³µ {ok_count} / ì‹¤íŒ¨ {len(errors)})")

            manifest_payload["updated_at"] = now_seoul_iso()
            if errors:
                manifest_payload["status"] = "error"
                manifest_payload["message"] = f"Uploaded with errors: {len(errors)} failed."
                manifest_payload["errors"] = errors
            else:
                manifest_payload["status"] = "queued"
                manifest_payload["message"] = "All files uploaded. Waiting for local worker."

            try:
                upsert_json_file(drive, folders["META"], manifest_name, manifest_payload)
            except HttpError as e:
                st.error("âŒ METAì— manifest ì €ì¥ ì‹¤íŒ¨ (Drive API)")
                st.write("HTTP status:", getattr(e.resp, "status", None))
                st.code(_http_error_details(e) or "(no error body)")
                raise

            st.session_state["active_batch_id"] = batch_id
            st.session_state["active_job_ids"] = [it["job_id"] for it in manifest_payload["items"]]

            if errors:
                st.warning(f"âš ï¸ ì¼ë¶€ ì—…ë¡œë“œ ì‹¤íŒ¨: {len(errors)}ê°œ")
                st.json(errors)
            st.success("âœ… ë°°ì¹˜ ì—…ë¡œë“œ ì™„ë£Œ")
            st.code(f"batch_id: {batch_id}")

st.divider()

# =========================================================
# 2) Status / Download (Batch)
# =========================================================
st.subheader("2) ìƒíƒœ í™•ì¸ & ZIP ë‹¤ìš´ë¡œë“œ")

# choose manifest
manifests = list_manifest_files(drive, folders["META"], limit=50)
default_idx = 0
selected_manifest = None

# if active_batch_id exists, try to preselect its manifest
active_batch_id = st.session_state.get("active_batch_id")
if active_batch_id:
    for i, f in enumerate(manifests):
        if f["name"].startswith(active_batch_id) and f["name"].endswith("__manifest.json"):
            default_idx = i
            break

if manifests:
    labels = [f'{f["name"]} (modified {f.get("modifiedTime","")})' for f in manifests]
    choice = st.selectbox("ë°°ì¹˜(manifest) ì„ íƒ", options=list(range(len(manifests))), format_func=lambda i: labels[i], index=default_idx)
    selected_manifest = manifests[choice]
else:
    st.info("manifest íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ë°°ì¹˜ ì—…ë¡œë“œë¥¼ ë¨¼ì € ì§„í–‰í•˜ì„¸ìš”.)")

if selected_manifest:
    st.caption(f"ì„ íƒëœ manifest: `{selected_manifest['name']}`")
    try:
        manifest = read_json_file_by_id(drive, selected_manifest["id"])
    except Exception as e:
        st.error("manifest ì½ê¸° ì‹¤íŒ¨")
        st.exception(e)
        st.stop()

    items = manifest.get("items", [])
    if not items:
        st.warning("manifestì— itemsê°€ ì—†ìŠµë‹ˆë‹¤. (ì—…ë¡œë“œê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ ì´ì „ ë²„ì „ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
    else:
        # Load meta per item
        rows = []
        terminal = True
        any_done = False
        done_targets = []  # (done_file name)
        for it in items:
            meta_name = it.get("meta_filename") or f'{it.get("job_id","")}.json'
            meta_obj = find_file_by_name(drive, folders["META"], meta_name)
            meta = None
            if meta_obj:
                try:
                    meta = read_json_file_by_id(drive, meta_obj["id"])
                except Exception:
                    meta = None

            status = (meta or {}).get("status", "unknown")
            progress = (meta or {}).get("progress", None)
            message = (meta or {}).get("message", "")
            done_file = (meta or {}).get("done_file", None)
            error_msg = (meta or {}).get("error", None)

            if status not in ("done", "error"):
                terminal = False
            if status == "done" and done_file:
                any_done = True
                done_targets.append(done_file)

            rows.append({
                "file": it.get("original_name") or it.get("inbox_name") or it.get("job_id"),
                "status": status,
                "progress": progress,
                "message": message,
                "done_file": done_file,
                "error": error_msg,
            })

        st.dataframe(rows, use_container_width=True)

        if auto_refresh and not terminal:
            try:
                from streamlit import st_autorefresh
                st_autorefresh(interval=refresh_sec * 1000, key="batch_poll")
            except Exception:
                pass

        st.markdown("#### 3) ZIP ë‹¤ìš´ë¡œë“œ")
        if not terminal:
            st.info("ì•„ì§ ëª¨ë“  íŒŒì¼ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (done/errorê°€ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
        elif not any_done:
            st.warning("ì™„ë£Œ(done)ëœ íŒŒì¼ì´ ì—†ì–´ì„œ ZIPì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # Build ZIP in memory
            if st.button("ğŸ“¦ ZIP ì¤€ë¹„í•˜ê¸°", type="secondary"):
                zip_buf = BytesIO()
                with zipfile.ZipFile(zip_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                    # Add a report
                    report = {
                        "manifest": selected_manifest["name"],
                        "generated_at": now_seoul_iso(),
                        "items": rows,
                    }
                    zf.writestr("report.json", json.dumps(report, ensure_ascii=False, indent=2))

                    for done_name in done_targets:
                        # find in DONE folder by name
                        done_obj = find_file_by_name(drive, folders["DONE"], done_name)
                        if not done_obj:
                            # keep note in report
                            zf.writestr(f"missing/{done_name}.txt", "DONE folderì—ì„œ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                            continue
                        data = download_file_bytes(drive, done_obj["id"])
                        zf.writestr(done_name, data)

                zip_bytes = zip_buf.getvalue()
                st.session_state["zip_bytes"] = zip_bytes
                batch_id = manifest.get("batch_id") or selected_manifest["name"].split("__manifest.json")[0]
                st.session_state["zip_name"] = f"{batch_id}.zip"
                st.success("ZIP ì¤€ë¹„ ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

            if st.session_state.get("zip_bytes"):
                st.download_button(
                    label="â¬‡ï¸ ê²°ê³¼ ZIP ë‹¤ìš´ë¡œë“œ",
                    data=st.session_state["zip_bytes"],
                    file_name=st.session_state.get("zip_name", "results.zip"),
                    mime="application/zip",
                    type="primary",
                )
                st.caption("ë‹¤ìš´ë¡œë“œ í›„ ìƒë‹¨ì˜ â€˜ğŸ†• ìƒˆ ì‘ì—… ì‹œì‘â€™ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìƒˆ ì‘ì—…ì„ ì§„í–‰í•˜ì„¸ìš”.")
